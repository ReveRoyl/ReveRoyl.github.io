<!DOCTYPE HTML>
<html lang="default,zh-CN,default">


<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="keywords" content="Thesis, Lei">
    <meta name="baidu-site-verification" content="fmlEuI34ir">
    <meta name="google-site-verification" content="yCy2azpds5XSuGZvis6OuA-XIGF5GuGpYRAaGfD6o48">
    <meta name="360-site-verification" content="b7c11a830ef90fd1464ad6206bb7b6e7">
    <meta name="description" content="
A novel convolutional neural network approach for classifying brain states under image stimuli
Lei Luo
Dr. Toby Wise
De">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>Thesis | Lei&#39;s Blog</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    <style type="text/css">
        
    </style>

    <script src="/libs/jquery/jquery-2.2.0.min.js"></script>
    <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?ce84511d3df71640a9378a69f6293044";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

    
        <script>
            (function(){
                var bp = document.createElement('script');
                var curProtocol = window.location.protocol.split(':')[0];
                if (curProtocol === 'https') {
                    bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
                }
                else {
                    bp.src = 'http://push.zhanzhang.baidu.com/push.js';
                }
                var s = document.getElementsByTagName("script")[0];
                s.parentNode.insertBefore(bp, s);
            })();
        </script>
    

    <script>
        (function(){
        var src = "https://jspassport.ssl.qhimg.com/11.0.1.js?d182b3f28525f2db83acfaaf6e696dba";
        document.write('<script src="' + src + '" id="sozz"><\/script>');
        })();
    </script>

<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>

<body>

    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Lei's Blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <i class="fa fa-home"></i>
            
            <span>Index</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <i class="fa fa-tags"></i>
            
            <span>Tags</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <i class="fa fa-bookmark"></i>
            
            <span>Categories</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <i class="fa fa-archive"></i>
            
            <span>Archives</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/about" class="waves-effect waves-light">
            
            <i class="fa fa-user-circle-o"></i>
            
            <span>About</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/contact" class="waves-effect waves-light">
            
            <i class="fa fa-comments"></i>
            
            <span>Contact</span>
        </a>
    </li>
    
    <li>
        <a href="#searchModal" class="modal-trigger waves-effect waves-light">
            <i id="searchIcon" class="fa fa-search" title="Search"></i>
        </a>
    </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Lei's Blog</div>
        <div class="logo-desc">
            
            To be or not to be, that is a question
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-home"></i>
                
                Index
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-tags"></i>
                
                Tags
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-bookmark"></i>
                
                Categories
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-archive"></i>
                
                Archives
            </a>
        </li>
        
        <li>
            <a href="/about" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-user-circle-o"></i>
                
                About
            </a>
        </li>
        
        <li>
            <a href="/contact" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-comments"></i>
                
                Contact
            </a>
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/ReveRoyl/ReveRoyl.github.io" class="waves-effect waves-light" target="_blank">
                <i class="fa fa-github-square fa-fw"></i>Follow Me
            </a>
        </li>
        
    </ul>
</div>

        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/ReveRoyl/ReveRoyl.github.io" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Follow Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = 'b1ab1e892617f210425f658cf1d361b5489028c8771b56d845fe1c62c1fbc8b0';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('Please enter the password to access this article')).toString(CryptoJS.enc.Hex)) {
                alert('Incorrect password, returning to the home page!');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/22.jpg')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        Thesis
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 20px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                        <a href="/tags/project/" target="_blank">
                            <span class="chip bg-color">project</span>
                        </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fa fa-bookmark fa-fw icon-category"></i>
                        
                        <a href="/categories/Neuroscience-programming/" class="post-category" target="_blank">
                            Neuroscience programming
                        </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                <div class="post-date info-break-policy">
                    <i class="fa fa-calendar-minus-o fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2022-08-24
                </div>

                <div class="post-author info-break-policy">
                    <i class="fa fa-user-o fa-fw"></i>Author:&nbsp;&nbsp;
                    
                    Lei Luo
                    
                </div>

                
                
                <div class="info-break-policy">
                    <i class="fa fa-file-word-o fa-fw"></i>Word Count:&nbsp;&nbsp;
                    11.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="fa fa-clock-o fa-fw"></i>Read Times:&nbsp;&nbsp;
                    69 Min
                </div>
                
                

                
                <div id="busuanzi_container_page_pv" class="info-break-policy">
                    <i class="fa fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                    <span id="busuanzi_value_page_pv"></span>
                </div>
                
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <p><img src="https://raw.githubusercontent.com/ReveRoyl/PictureBed/main/BlogImg/202208251426116.png" alt="LuoLei_Poster2022_Page_2"></p>
<p><strong>A novel convolutional neural network approach for classifying brain states under image stimuli</strong></p>
<p>Lei Luo</p>
<p>Dr. Toby Wise</p>
<p>Department of Neuroimaging<br>Institute of Psychiatry, Psychology &amp; Neuroscience<br>King’s College London<br>University of London</p>
<p><strong>Thesis in partial fulfilment for the degree of MSc in Neuroscience September, 2022.</strong></p>
<h1 id="Personal-Statement"><a href="#Personal-Statement" class="headerlink" title="Personal Statement:"></a>Personal Statement:</h1><p>The study was designed by Lei Luo under the supervision of Dr. Toby Wise. MEG data was from Wise et al. (2021). The thesis was written entirely by Lei Luo, with language corrections and suggestions from Dr. Toby wise. Any research or work mentioned in the paper has been fully and accurately cited. Computation resource is provided by King&#39;s Computational Research, Engineering and Technology Environment (CREATE) (King’s College London, 2022). The neural network code is using machine learning library Pytorch (Paszke et al., 2017). Statistics are done with IBM Spss. Topographical maps are generated using library MNE-Python. Code availability: MEG data used in this research in available at <a href="https://openneuro.org/datasets/ds003682" target="_blank" rel="noopener">https://openneuro.org/datasets/ds003682</a>; and all analysis code in available at <a href="https://github.com/ReveRoyl/MT_ML_Decoding" target="_blank" rel="noopener">https://github.com/ReveRoyl/MT_ML_Decoding</a>.</p>
<h1 id="Abbreviations"><a href="#Abbreviations" class="headerlink" title="Abbreviations"></a>Abbreviations</h1><p>CBAM Convolutional block attention module</p>
<p>CNN Convolutional neural network</p>
<p>ECoG Electrocorticography</p>
<p>EEG Electroencephalography</p>
<p>ICA Independent Components Analysis</p>
<p>MEG Magnetoencephalography</p>
<p>MLP Multilayer perceptron</p>
<p>MRI Magnetic resonance imaging</p>
<p>fMRI Functional magnetic resonance imaging</p>
<p>FC Fully connected</p>
<p>LSTM Long short-term memory</p>
<p>RNN Recurrent Neural Network</p>
<p>RPS Relative power spectrum</p>
<p>PCA principal component analysis</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Background: The mechanism of human neural responses to different stimuli has always been of interest to neuroscientists. In the clinical situation, tools to distinguish different diseases or states are required. However, classic classification methods have obvious shortcomings: traditional clinical categorical methods may not be competent for behaviour prediction or brain state classification and traditional machine learning models are improvable in classification accuracy. With the increasing use of convolutional neural networks (CNN) in neuroimaging computer-assisted classification, an ensemble classifier of CNNs might be able to mine hidden patterns from MEG signals. However, developing an effective brain state classifier is a difficult task owing to the non-Euclidean graphical nature of magnetoencephalography (MEG) signals.</p>
<p>Objective: This project had two aims: 1) to develop a CNN-based model with better performance in classification than traditional machine learning models; 2) to test if the model can be improved with extra information adding relative power spectrum.</p>
<p>Methods: To address this brain state classification modelling issue, I used MEG signals from 28 participants viewing 14 image stimuli to train the CNN. The CNN subsequently underwent 10-fold cross-validation to ensure proper classification of MEG. I also extracted the relative power spectrum and provided this to the network. The following main techniques were applied in this research, principal component analysis (PCA), convolutional block spatial and temporal features extracting modules, convolutional block attention module (CBAM) techniques, relative power spectrum (RPS) techniques, fully connected (FC) techniques.</p>
<p>Results: In this research, my method was applied to the MEG dataset, the average classification accuracy is 23.07%±7.69%, which is much better than the baseline models: LSTM RNN model 15.38% (p = 6.8 × 10 ^–2^) and simple image classification CNN model 11.53% (p = 5.9 × 10 ^–2^). Relative power spectrum information (mainly beta and delta during this task) successfully informed the model improving its performance.</p>
<p>Conclusion: These results demonstrate that my method is feasible for the analysis and classification of brain states. It may help researchers diagnose people in the clinical situations and inform future neurological classification approaches in regard to higher specificity in identifying brain states.</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="Machine-learning-in-medical-utilisation"><a href="#Machine-learning-in-medical-utilisation" class="headerlink" title="Machine learning in medical utilisation"></a>Machine learning in medical utilisation</h2><p>Since Donald Hebb first composed the cell assembly theory stating the consistency between neuronal activity and cognitive processes (Brown &amp; Milner, 2003; Shaw, 1986), the idea of neural networks started to stand out in public visibility. Until Frank Rosenblatt first developed and explored the basic ingredients of deep learning (DL) (Tappert, 2019), the only constraint that can slow our steps are applications of math methods. The last past decades have seen the quick and great revolution of artificial intelligence as the development of computer science. What the big breakthrough takes us close to the scientific field are the powerful tools that teach machines to learn about the physical world. Even though machine learning techniques have gradually built their presence in recent decades, the application in medical utilisation lags.</p>
<p>In the past centuries, neuroscientists have always been attempting to classify and predict the brain’s response to the visual world. Recently, with the rapid emergence of novel non-invasive techniques such as magnetoencephalography (MEG), electroencephalography (EEG) and magnetic resonance imaging (MRI), neuroscientists start to use these tools to solve the historical conundrum; and have made huge progress in the visual perceptual decoding or so-called “brain-reading” field (W. Huang et al., 2020). Each individual conscious experience is associated with a unique brain activity pattern so that it can be regarded as a fingerprint of specific brain activity. It is theoretically viable to read out one’s current idea with a specifically designed computer vision and neuroimaging pattern (Haynes, 2012). In this case, it might be possible that these “brain-reading” techniques can tell us that could be helpful with regard to clinical applications. Nowadays, many neural mechanisms have been elucidated. Although the current studies are mostly on a reflective proof of concept track (Hedderich &amp; Eickhoff, 2021), it is promising that these approaches will pave the way and build a solid foundation in regard to clinical applications. In contrast to the classic understanding of some mental mechanisms, more and more researchers believe that traditional categories systems may twist the real cause of diseases or behaviours (Bzdok &amp; Meyer-Lindenberg, 2018). To fill this gap, deep learning techniques have been introduced into disease diagnosis and classification. It can avoid being affected by people’s opinion bias but conversely get feedback from people so as to improve its learning ability from existing experience (Currie et al., 2019). Apart from disease classification, it is more profound to study human brain states under different conditions. Further studying of future state simulation has properly gained attention, especially on episodic future thought: the ability to rehearse events in mind, which may be going to happen in one’s life trajectory (Schacter et al., 2017; Szpunar, 2010).</p>
<h2 id="Aversive-state-reactivation-and-replay"><a href="#Aversive-state-reactivation-and-replay" class="headerlink" title="Aversive state reactivation and replay"></a>Aversive state reactivation and replay</h2><p>The aversive state is critical for harm avoidance, playing a vital role in wilderness survival and social life (Terranova et al., 2022). As part of the aversive state, the observational fear process promotes one’s capability of showing empathic fear when seeing other’s aversive situations. This process may benefit from the neural replay and reactivation of individuals. The process that current state simulation reinforces the existing memory network is called “reactivation” while the neural activity activation is named “neural replay”. The reactivation is based on past experience and in turn, promotes the storage of it, as well as facilitating the planning, inference and reward values updating (Wimmer &amp; Shohamy, 2012; Wise et al., 2021). As mentioned in the previous section, one way to look at future thought simulation is to investigate memory reactivation.</p>
<p>Recent works have shown that neural replay and reactivation are prior important in avoidance behaviour (Wu et al., 2017), which may provide individuals with a prospective prediction based on the possible consequence simulation (Doll et al., 2015). Since then, it is known “which” correlates to the aversive state, the following step is figuring out to what extent neuronal activity is associated with behaviour. Naturally, it encourages researchers to try predicting one’s avoidance behaviour with the neuroimaging data recording. In recent years, more and more studies start to use neuroimaging classification to look for the inner mechanisms of brain states’ reactivation (Belal et al., 2018; Eichenlaub et al., 2020; Roscow et al., 2021). If we want to look at memory reactivation, what we need is really good decoding methods with neuroimaging data recordings. So, it’s important to optimise existing decoding methods as far as possible, which will set the scene for future related work.</p>
<h2 id="Magnetoencephalography"><a href="#Magnetoencephalography" class="headerlink" title="Magnetoencephalography"></a>Magnetoencephalography</h2><p>Magnetoencephalography (MEG) is useful for detecting brain states and evaluating the behavioural response. It allows us to map and locate specific brain areas and ongoing functions (Bunge &amp; Kahn, 2009). The principle of MEG is based on magnetic induction. It is widely known that when neurons are activated, electrical signals will be generated synchronously. According to the magnetic induction principle, when the electrical fields change, secondary magnetic fields are generated. The brain-evoked magnetic field strength is usually in the range of femto-tesla to pico-tesla, i.e., 10–15 to 10-12 tesla (Singh, 2014). With the precise MEG device and mathematical preprocessing methods, these tiny signals are able to be separated from the noise and collected. MEG records magnetic fields, from which can be inferred changes in the transmission of postsynaptic current between cortical neurons.</p>
<p>Compared to other neuroimaging methods, functional magnetic resonance imaging (fMRI) has high spatial resolution but a low temporal resolution (Dash, Sao, et al., 2019); electroencephalography (EEG) records the electrical field that may twist between skin and skull, and EEG is based to reference point location hence it is sensitive to small measurement error. Electrocorticography (ECoG) is an invasive method, so it is not suitable for healthy participants and some patients; MEG stands out for its higher spatial, temporal resolution and dynamic time sequentiality. At the same time, MEG, as a non-invasive method, has its specific advantage: low preparation time, which supports a possibility for most clinical conditions. Furthermore, as novel portable MEG devices come out, it creates opportunity for various ages participants and patients (Boto et al., 2018). MEG data records complex high-dimensional information about the brain network and the responding source locations, which is hard to collect with classic classification methods (Giovannetti et al., 2021). However, in the analysis period, it is a burden for researchers to do classification with MEG data: it is complex to correctly extract required signals during preprocessing, and lots of related experience is required when dealing with complex sensors and waveform patterns. Hence deep learning is expected to lighten the load of researchers, add the universal applicability of classification and increase the prediction accuracy. In this case, it is a challenge to choose the proper neural network.</p>
<h2 id="Convolutional-Neural-Network"><a href="#Convolutional-Neural-Network" class="headerlink" title="Convolutional Neural Network"></a>Convolutional Neural Network</h2><p>The CNN is a particular subtype of the neural network, which is effective in analysing images or other data containing high spatial information (Khan et al., 2018; Valueva et al., 2020) and also works well with temporal information (Bai et al., 2018). The same as the real neural networks in the brain, neurons in CNN process limited input data in a restricted receptive field and cooperate with each other by overlapping to cover the whole visual space (the filter extracting the features is called kernels). It is an automatic feature extraction process. Thus, it is now necessary to manually design feature extraction algorithms, which is required in traditional machine learning algorithms. It is also the main advantage of CNN to learn features from input data: avoiding the impact of artificial artefact in the algorithm design step. The main specialization of CNN is clearly the convolution part, which is a linear mathematic operation allowing extracting the nearby features of input data. The convolutional operation generates a series of machine-recognizable output features. It is suggested that the convolutional layer can be considered as a graphical pattern mining or feature extraction process (Li et al., 2018). What is more, previous research has shown the ability of CNN as a tool for analysing MEG data, which is in detail classifying brain activity and identifying potential neural sources (Zubarev et al., 2019). The process of generating the features map is following the sequential architecture, which is not like a cyclical recurrent neural network. A CNN model is usually composed of the input layer (the first layer where input data is passed in), multiple convolutional layers (feature extraction layers), alternative pooling layers (downsampling layer for feature maps), fully connecting layers (used to map the feature vectors obtained from previous feature extraction layers to the next layer), and output layers (the final layer where predictions are made). Inside each layer, activation functions are optional. A simple CNN network is shown as an example in figure 1. The convolutional layer receives input data, apply the convolution process to the data, and passes data to the following step. In the convolution function shown in equation 1, the f stands for input data; k stands for kernel filter, m, n respectively stands for the result matrix rows and columns index:</p>
<p>$$\begin{matrix}<br>G\lbrack m,\ n\rbrack = \ (f*k)\lbrack m,\ n\rbrack = \ \sum_{i = 1}^{m}\ \sum_{j = 1}^{n}\ \ k\lbrack i,\ j\rbrack f\lbrack m - i,\ n - j\rbrack\ \ (1) \<br>\end{matrix}$$</p>
<p>As shown in figure 2 (A), a kernel filter is applied to the input data pixel: after summing up input values and filter, a result value is generated and passed to the next step. With all similar processes conducted step by step, a feature map is generated. Afterwards, the max pooling step (figure 2 (B)) comes to decrease the dimensions of data in order to keep more neurons activated which is reported to reduce the overfitting as well (Y. Huang et al., 2015).</p>
<p><img src="https://raw.githubusercontent.com/ReveRoyl/PictureBed/main/BlogImg/202208241717167.png" alt="image-20220824171734112"></p>
<p><strong>Figure 1.</strong> A simple CNN architecture illustration (5 convolutional layers and pooling layers, 3 fully connected layers)</p>
<p><img src="https://raw.githubusercontent.com/ReveRoyl/PictureBed/main/BlogImg/202208241717946.png" alt="image-20220824171744899"></p>
<p><strong>Figure 2</strong>. Convolution illustration (A), a 3*3 kernel filter (blue) is applied to a 6*6 input data (red) and give an output (green) as 4*4 (feature map). Max pooling illustration (B), data transformation is processed from 4*4 input to 2*2 output.</p>
<p>In previous research, sequence data were usually analysed with the recurrent neural network (RNN) while the convolutional neural network (CNN) was used for image prediction. However, recent works have demonstrated the effectiveness of CNN in time-sequential data (Bai et al., 2018) where CNN even have a longer effective memory. It provides us with the theoretical basis for utilizing various CNN models in behaviour prediction. CNN has also been used for MEG classification in recent years: previous research has successfully predicted different diseases such as brain tumours (Rajasree et al., 2021) and Alzheimer’s disease (Aoe et al., 2019; Giovannetti et al., 2021). In fact, some studies have shown that CNN offers an unreplaceable advantage for patterns modelling those other techniques may not be disposed to reveal (Giovannetti et al., 2021). However, it is still an emergent topic to use deep learning instead of the classic machine learning method. As a special machine learning technique, deep learning benefits from the fast development of high-performance computation (HPC). One example is that deep learning can use the CUDA framework to accelerate training. As the GPU accelerators become more and more performance and energy-consuming effective (Faraji et al., 2016), the cheaper computation source becomes more and more available. Some evidence has shown that deep learning performs better than the classic machine learning methods when doing MEG classification (Aoe et al., 2019; RaviPrakash et al., 2020; Zheng et al., 2020). In addition to these the ability to recognize temporal and spatial data patterns, CNN has the unique character of sharing weight among neurons in a convolutional layer (Anelli et al., 2021). In this case, the parameters quantity reduces sharply which benefits analysing complex structured MEG data.</p>
<h2 id="Band-power-and-Transfer-learning"><a href="#Band-power-and-Transfer-learning" class="headerlink" title="Band power and Transfer learning"></a>Band power and Transfer learning</h2><p>CNNs are expected to have good performance for extracting features from MEG data, but the performance can be boosted by augmenting the data we feed into them. Here are some ways I did this with MEG data. MEG signals reflect brain activity, in which the brainwave can be deposed into different frequency power bands, such as delta (0.5-4 Hz), theta (4-8 Hz), alpha (8-12 Hz), beta (12-30Hz), and gamma (above 30 Hz). And these different brainwave frequency bands usually are associated with different brain states, such as the alpha band has been implicated in visual attention (Rohenkohl &amp; Nobre, 2011). and the beta band is usually associated with anxiety (Einöther et al., 2013). That is why I think particular frequency bands might be important in helping classify accurately. In order to understand if it is available to extract critical information from the aversive state, it may be viable to extract different power bands and analyse them as a neural activity representation. One drawback of this method is it may reduce the available temporal information in MEG data, but it can not be denied that it provides an excellent and reliable method to study brain states (Newson &amp; Thiagarajan, 2019).</p>
<p>However, the trained model may not be suitable for data collected under other conditions. Between the MEG device and its recording source location, there are the skull, skin and even air, which may all affect the signals we get: the deeper source is, the more affected it will be. In addition to these confounds, the geometric shape of the skull which varies a lot between different people, also affects a lot (Hagemann et al., 2008). In order to generalize the model, power spectrum standardization and relative power computation are necessary. Moreover, it is reported that transfer learning can help to improve learning efficiency by reusing or transferring learnt parameters (Karimpanal &amp; Bouffanais, 2018). In this case, transfer learning may reduce the influence of these confound factors and increase the generality of models.</p>
<p>Transfer learning is the machine learning technique which allows a network to learn in one condition and improve its performance under another relevant condition. It is an optimization method to inform new task learning with relative learnt knowledge (Soria Olivas &amp; IGI Global., 2010). For the transfer learning process, only part of the model parameters is trained and adjusted, which is called “tuning”. If all network parameters are opened for training, it is easy to fall into the state of overfitting the target training set, thereby reducing the generalization performance of the model. The first few layers of the network are generally used for feature extraction. If the difference between the source task and the target task is not quite significant and the model has achieved good performance in the source task, there is no need to perform training from the beginning (Karimpanal &amp; Bouffanais, 2018). In this case, the transfer learning technique solves the small data availability problem: with a small amount of data, it helps eliminate the overfitting problem and reduce the model training time. Some evidence suggests that it can keep at a high accuracy level and saves 90% time (Dash, Ferrari, et al., 2019). It has been widely used to transfer the weight or bias of the current network to a newly trained network with the object of faster convergence and better performance.</p>
<p>For aversive state reactivation prediction, previous studies have provided an approach with logistic regression methods and get a good accuracy (Wise et al., 2021) With a different approach (CNN) we could probably detect brain state reactivation with much better accuracy and learn a lot more about it. My first aim is to optimize the model with new techniques such as CNN and apply spectrum power. The second aim is to generalize the model by informing one participant model with other participants’ data. I assume that: first, the performance of the CNN model is better than traditional machine learning models; second, adding the power spectrum to augment data will improve the performance as well.</p>
<h1 id="Materials-and-Methods"><a href="#Materials-and-Methods" class="headerlink" title="Materials and Methods"></a>Materials and Methods</h1><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>The participants, study design, data collection and preprocessing sections and relevant information are included and published in the paper &quot;Model-based aversive learning in humans is supported by preferential task state reactivation&quot; (Wise et al., 2021). Open access to the original MEG data can be found in the public repository: <a href="https://openneuro.org/datasets/ds003682" target="_blank" rel="noopener">https://openneuro.org/datasets/ds003682</a>. In total 28 participants took the task. The task is designed as follows: participants were required to sit with the MEG device in front of a monitor, where 14 images were shown. The recording duration of each stimulus is 1.29 seconds (from 0.5 before to 0.79 after image is shown). MEG data were collected with CTF 275-channel axial gradiometer system (CTF Omega, VSM MedTech). In the preprocessing session, the Maxwell filter was applied to remove noise firstly. Then a high pass filter above 0.5 Hz and a low pass filter below 45 Hz were applied. Afterwards, signal components were separated with independent component analysis (ICA) to isolate noise-related components in the setting of finding components which explain 95% variance. To reduce information loss, MEG data was upsampled and the window width of data was set as 800 time points (Aoe et al., 2019) when training the model to get better performance.</p>
<h2 id="Power-spectrum-extraction"><a href="#Power-spectrum-extraction" class="headerlink" title="Power spectrum extraction"></a>Power spectrum extraction</h2><p>In order to compute the relative power (percentage power) of MEG signals, the first step is to extract the power band with the specific frequency. The frequency bands were chosen to be delta (0.5-4 Hz), theta (4-8 Hz), low alpha (8-10 Hz), high alpha (8-12 Hz) beta (12-30 Hz), and gamma (above 30 Hz). Actually, the gamma frequency was below 50 Hz because it is impossible to detect any information above 50 Hz as the data are sampled at 100 Hz.</p>
<p>Firstly the power spectral densities (PSD) and frequency of each band were derived using Welch’s method with mne.time_frequency.psd_array_welch() function provided by MNE-Python (Percival &amp; Walden, 1993; Slepian, 1978). The reason for choosing this function is this function gives a single value for each trial but other methods that give you power at each timepoint. Then the PSD of each band was integrated with the frequency as spacing point using composite Simpson’s rule. The absolute power in a specific location is the average number of the power from several adjacent electrodes. The relative power is the ratio of the absolute power of a band to the total band power in all frequencies. As shown in equation 2, the r represents the relative power of a frequency band, the a stands for absolute power of the same frequency band, Pi ‘s are the absolute power in all frequency bands:</p>
<p>$$\begin{matrix}<br>r = \frac{a}{P_{t}} = \ \frac{a}{\sum_{\ }^{\ }{P_{i}\ }}\ \ (2) \<br>\end{matrix}$$</p>
<p>Eventually, the absolute and relative power bands are transformed from input data in $\mathbb{R}$C×T (where $\mathbb{R}$ is a vector space) to $\mathbb{R}$C×F, where C is the number of channels, T is time points and F is the number of frequency bands, i.e., 6 here.</p>
<h2 id="Neural-Network-architecture"><a href="#Neural-Network-architecture" class="headerlink" title="Neural Network architecture"></a>Neural Network architecture</h2><p>In order to classify different 14 categories brain states under 14 stimuli based on MEG signals, I proposed a CNN model ASRCNet-v1. The input data are MEG recordings from 24 subjects (4 were removed because of its information missing). In order to augment the data, the input of last fully connected layer was concatenated with relative power bands in all frequencies. The neural network structure of ASRCNet-v1 was developed based on the previously reported model EnvNet-v2 and MNet (Aoe et al., 2019; Tokozume et al., 2017; Tokozume &amp; Harada, 2017), which was used to classify environmental sounds and Alzheimer’s diseases. The detailed configuration of ASRNet-v1 is as demonstrated in figure 3 and the data processing is demonstrated in figure 4. There are three convolutional blocks in total: two feature extracting blocks: spatial and temporal blocks; and a CBAM block after them. In the first convolutional layer, the global features were extracted with a large filter, which has the same kernel width as the channel number of inputs. The kernel length of the first layer is set to be 64. The first layer generates a feature map in $\mathbb{R}$S×1×T’ from input data in $\mathbb{R}$C×T, where the T is larger than T’s. The C (number of channels) is larger than S (number of spatial filters) such that the channel dimension is reduced. The second convolutional layer in the spatial block generates a feature map in $\mathbb{R}$S’×1×T’’ with frequency features then. Afterwards, the data is downsampled with a max pooling layer and swapped along the axis between S and 1, i.e., from $\mathbb{R}$S’×1×T’’ to $\mathbb{R}$<code>&lt;!-- --&gt;</code>{=html}1×S’×T’’. This operation allows data being considered as the image changing the convolutional direction (Tokozume et al., 2017). The second block consists of eight convolutional layers and four max pooling layers. The kernel size of convolutional layers is small and decreases every two layers in order to extract local frequency temporal features from the output feature map from the previous layer. Relu is the activation function for all convolutional layers in the spatial block. Max pooling layers are attached after every two convolutional layers.</p>
<p>The attention block is composed of 2 parts: the channel attention module and the spatial attention module. Two modules help model focus more on the important information: channel dimension and spatial dimension. First, for the channel attention module, input data process average pooling and max pooling separately, where the average pooling layer is used to aggregate spatial information and the max pooling layer is used to maintain more extensive and precise context information as images’ edges. The outputs are passed to an MLP (multilayer perceptron) network with the same weight. The MLP layer has a bottleneck. The width and length or the number of neurons in this MLP layer are decided by a reduction ratio of 16. Then the sum of two outputs from the MLP layer is given to the sigmoid activation function in order to project values in features map into $\mathbb{R \in}(0,1)$. Finally, the channel attention module returns a feature map as the product of input and calculated scale. Compared with the channel attention module, spatial attention seems to be simpler, which includes one convolutional layer and the sigmoid activation function. The same as the channel attention module, the spatial attention module returns a feature map as the product of input from the previous module and calculated scale in this module.</p>
<p>Then the first fully connected layer (FC) comes to play the role of classifier, which is projecting the “distributed feature representation” of the feature map to sample labels space $\mathbb{R}$L, where L is the number of features. In the object of data augmentation, the input is concatenated with relative power bands in all frequencies. The following step is another FC layer, which finally generates 14 features corresponding to the number of categories. At last, data is passed to the softmax activation function to convert the numbers vector into the probabilities vector. To avoid overfitting, 30 % dropout is applied after the last 4 spatial convolutional layers and 50 % dropout is applied after the first fully connected layer (Srivastava et al., 2014); batch normalization is applied to boost the speed of learning (Ioffe &amp; Szegedy, 2015).</p>
<p><img src="https://raw.githubusercontent.com/ReveRoyl/PictureBed/main/BlogImg/202208241718131.png" alt="image-20220824171834060"></p>
<p><strong>Figure 3</strong>. Detailed configuration of ASRCNet-v1. Cov: convolution; Relu: rectified linear unit; MaxPool: max pooling; AveragePool: average pooling; Concat: concatenation; Identity: stands for relative power spectrum; Gemm: general matrix multiply</p>
<p><img src="https://raw.githubusercontent.com/ReveRoyl/PictureBed/main/BlogImg/202208241718214.png" alt="image-20220824171849114"></p>
<p><strong>Figure 4.</strong> Chart flow of data processing. The original data is shaped as (64,1,272,800) where 64 is the number of batches, 272 is the number of channels and 800 is the number of time points.</p>
<h2 id="Model-training-and-testing"><a href="#Model-training-and-testing" class="headerlink" title="Model training and testing"></a>Model training and testing</h2><p>The input data is a total of 900 epochs of 800-time-point MEG signals from 272 channels. At the beginning, the data was separately processed directly with the neural network and the Fourier transformation. The latter process provides neural network with relative band power in frequency delta (0.5-4 Hz), theta (4-8 Hz), alpha (8-12 Hz), beta (12-30Hz), and gamma (above 30 Hz). The input data is shuffled and scaled with variance scaling method, which reasonably preserves the dynamic range of data, before being fed into the model. The first step is to normalize all input data for the reason that normalization step can generalize the statistical distribution of uniform samples, which is expected to enhance the training performance. The normalization process is as shown in equation 3, where m is the total number of data and x represents data, makes the average value and standard deviation of data in each channel to be located in the range between 0 and 1:</p>
<p>$$\begin{matrix}<br>{x_{i}}^{‘} = \frac{\left| x_{i} - \frac{1}{m}\sum_{1}^{m}\ x \right|}{\sqrt{\sum_{1}^{m}\ x^{2}}}\ \ (3) \<br>\end{matrix}$$</p>
<p>In every training period, the input is one piece of a small segment of 64 batches of MEG signals segmented with non-overlapped 800-time-point time windows. The cross-entropy loss function was chosen to train the model because of its better performance in computing losses for discrete distributions. I chose SGD to be the optimizer because it is reported to have a better generalization capacity compared with Adam even though it may converge slower (Hardt et al., 2015). For the SGD optimizer, I set the initial learning rate as 0.0005, and momentum to 0.9. Specifically for the parameters in the second FF layer, a weight decay as 0.0005 is set keeping away from overfitting. The initial weight is randomized, which is because it is reported there is no obvious performance promotion with manually weight initialization (Hoshen et al., 2015). In order to improve training efficiency and avoid overfitting, I adapted the update step: I used the dynamic learning rate when the valid loss approaches a plateau (function 4, where $\lambda$ represents learning decay and L represents valid loss). The patience of the dynamic learning rate is set to 1, the threshold is set to 0.001 and learning rate decay is set to 1e-8. Since there are a large number of features during training, in order to avert overfitting, I introduced L2 regularization (ridge regularization) with the regularization parameter lambda to 0.001. After the trial with model performance in different checkpoints, early stopping was finally adopted when the number of epochs reaches around 130 in case of overfitting.</p>
<p>$$\begin{matrix}<br>\alpha_{t + 1}\  = \ \left{ \begin{matrix}<br>\alpha_{t} \times \lambda &amp; if\ L_{t + 1} &gt; \ \ L_{t} \<br>\alpha_{t} &amp; if\ L_{t + 1} \leq \ \ L_{t} \<br>\end{matrix} \right.\ \ \ (4) \<br>\end{matrix}$$</p>
<p>Finally, the possibility of each label is generated in the model. It eventually gives only one “most possible” label after comparing the possibility in all labels. In order to prove the validity of this CNN model. ASRCNet-v1’s performance is evaluated with 10-fold cross-validation, where 1 in 10 sets is used as validating set every time.</p>
<h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><p>The key results of the article are summarised and given in this section. This section first offers auxiliary findings with the whole dataset and furthermore demonstrates results using a single MEG dataset (i.e., a single subject). The accuracy measure is utilized to compare the performance of various models. All models had previously undergone testing on participant 1 to provide an initial indication of performance. The suggested model solution is validated across all of the participants once all models have been evaluated on participant 1. Additional tests are specifically run on the best model ASRCNet and other two main baseline models (LSTM RNN and simple CNN). For models tested on all subjects, model training was done within each participant, trained and evaluated only using its individual recording MEG data. Additionally, I set the windows of input data to 800 ms enabling an accurate comparison of several test models. Relative power spectrum is introduced to the training improving performance of models. Moreover, average topographical maps are graphed as the representation of the MEG signals intensities in the anatomical brain, illustrating the general topographic maps of brain states from different stimuli in brain states reactivation tasks.</p>
<h2 id="Topographical-map"><a href="#Topographical-map" class="headerlink" title="Topographical map"></a>Topographical map</h2><p>The global anatomical brain maps are generated with concatenated dataset and were back fitted to MEG recordings. The average MEG signals with different stimuli are calculated and graphed showing commons and differences among various brain states under different stimuli. In the brain states reactivation process, under the different stimuli, the topographical maps all look very similar. There are subtle differences but in general the pattern is the same: similar brain regions are activated while to the different extent (figure 5). This means we need some kind of algorithm that is sensitive to these very small differences between stimuli.</p>
<p>The result shows the rationality of extracting features of brain states under different stimuli. It may suggest that stimulus representations are in the downstream temporal region or visual cortex. Thus, as the following training results showed, ASRCNet is an effective and reasonable approach to classifying these states.</p>
<p><img src="https://raw.githubusercontent.com/ReveRoyl/PictureBed/main/BlogImg/202208241719153.png" alt="image-20220824171924099"></p>
<p><strong>Figure 5</strong>. brain topographical map under different stimuli in the specific time (0.36 s, 0.79 s after giving the stimuli). The average brain states of all 24 subjects in all 14 stimuli are shown as the topographical map. The map shows these different brain states as an intensity map, where the red colour shows stronger intensity and blue shows weaker intensity. The brain areas that are activated are concentrated in the downstream temporal region or visual cortex.</p>
<h2 id="Power-spectrum"><a href="#Power-spectrum" class="headerlink" title="Power spectrum"></a>Power spectrum</h2><p>In order to determine the effect of different brain wave frequencies, power spectral density (PSD) at all 272 channels is calculated. The 6 power bands are divided by the sum generating 1632 decoding features (272 channels for each of the 6 frequency bands). I analysed the power spectrum in all frequencies of input 800-time-point MEG signals for different participants. The results show that beta and delta waves are in the large and major proportion (figure 6). It may be considered as potential evidence that beta and delta waves are associated with not only anxious thinking, and active concentration (Baumeister et al., 2013), but also the aversive state. In the following classifier task, these findings are in line with results showing the involvement of beta and delta in concentration.</p>
<p><img src="https://raw.githubusercontent.com/ReveRoyl/PictureBed/main/BlogImg/202208241721133.gif" alt="img"><img src="https://raw.githubusercontent.com/ReveRoyl/PictureBed/main/BlogImg/202208241720900.gif" alt="img"></p>
<p><strong>Figure 6.</strong> Relative and absolute power spectrum (average of 24 subjects), beta (12 to 30 Hz) and delta (0.5 to 4 Hz) waves are in the major proportion</p>
<h2 id="Classification-of-multiple-brain-states-reactivations"><a href="#Classification-of-multiple-brain-states-reactivations" class="headerlink" title="Classification of multiple brain states reactivations"></a>Classification of multiple brain states reactivations</h2><p>In order to classify the brain states reactivation with different given image stimuli (figure 7), ASRCNet-v1, the classifier is developed. It is trained with MEG signals for each of the images. The representative MEG signals for each reactivation states that ASRCNet-v1 accurately identified is displayed in figure 8. A sample of an 800-time-point segment of the preprocessed MEG signals is displayed in the panel, where each contains 900 epochs. In these samples, there are no spikes or other distinctive abnormal waveforms. For every single training dataset from various participants, 900 of in total 900 events passed the rejection process. Therefore, none of these signals are removed because of bad channels. It is because the rejection algorithm was purposefully designed to be inclusive. All data were deliberately included because the CNN model should be robust to noise in the data. It is suggested that ASRCNet-v1 correctly categorised the MEG signals during aversive state by utilising features that not presents in the typical classification.</p>
<p><img src="https://raw.githubusercontent.com/ReveRoyl/PictureBed/main/BlogImg/202208241721069.png" alt="image-20220824172119017"></p>
<p><strong>Figure 7.</strong> image stimuli in different brain states reactivation tasks (Wise et al., 2021), from left to right, above to bottom are labelled as stimulus_i.</p>
<p><img src="https://raw.githubusercontent.com/ReveRoyl/PictureBed/main/BlogImg/202208241721550.png" alt="image-20220824172132497"></p>
<p><strong>Figure 8.</strong> representative MEG signal which is classified by the ASRCNet-v1 (from one sample participant)</p>
<p>Principal component analysis (PCA) (30 to 50 out of 272 channels) was tested to be applied to the input data before feeding data into the model. But the result shows that PCA does not obviously improve the classification performance of ASRCNet-v1. The classification accuracy was not obviously affected when PCA was applied. In the beginning, I didn&#39;t get satisfactory learning convergence results. When the input data values are clipped to be in of standardized bounds, the learning curves became smooth and the valid loss gradually decreased step by step.</p>
<p>Since there are 14 stimuli and the chances for all stimuli are equal, the random prediction accuracy is expected to be 7.14% (1/14 = 7.14%). The classification accuracy of ASRCNet-v1 is around 23.07%, which is clearly higher than random chance. LSTM RNN gives an accuracy of about 15.38% while simple CNN only gives a mean accuracy of 11.53%. I compared the performance of different models (figure 9) and found it is suggested that ASRCNet-v1 outperformed any other simple approach (LSTM RNN, simple CNN with 2 convolutional layers and 1 pooling layer). Compared with the other classification approach, ASRCNet-v1 exhibits the best classification performance (p = 6.8 × 10 –2 for LSTM RNN, p = 5.9 × 10 –2 for CNN, paired Wilcoxon signed-rank tests). The best classification accuracy of ASRCNet-v1 is able to reach 33.33%. The classification accuracy variety between simple CNN and ASRCNet may suggest that the classification depends on some event-independent signal: the temporal and spatial features. To which extent the performance is a result of the detected differences in the temporal and spatial features of MEG signals, remained to be explored.</p>
<p><img src="https://raw.githubusercontent.com/ReveRoyl/PictureBed/main/BlogImg/202208241721928.png" alt="image-20220824172150882"></p>
<p><strong>Figure 9.</strong> performance of different model boxplot. The boxplot shows the classification accuracy of different models. The random chance baseline is 7.14% (1/14 = 7.14%). All models learned from MEG signals (all accuracies are above 7.14% and gives better predictions. ASRCNet gives the best classification accuracy as 23.07%±7.69% (mean ± standard deviation).</p>
<h1 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h1><p>In this section, I am going to discuss the experimental results, findings, and potential future work. The focus of this paper is to explore the potential of deep learning, especially CNN to classify the aversive brain states associated with visual images. The MEG signals are complex and have low signal-to-noise ratios data structures. What is more, the aversive brain states parameters are continuous variables. Therefore, the aim is to solve a complex classification task. To optimally address this problem, a CNN model ASRCNet (inspired by Mnet and EnvNet, which was used in MEG data) is proposed as the deep learning solution in this research because of its ability to extract complex patterns from raw input data. One focus is to understand whether the CNN model is able to perform the decoding task. The analysis has been evaluated in two distinct steps: First, two well-known architectures (LSTM RNN and CNN: the former model is known as a good fit for sequential data and the second model is usually used in image classification) were selected as baseline models for the purpose of understanding the potential of general-purpose DL models that are not specific to MEG analysis. Second, I test the prediction and classification ability of the CNN-based architecture specially designed for MEG recordings. The results show that ASRCNet provide better performance compared to the other two baseline models. Since the model is specifically designed to extract features from MEG recordings, it can be considered an option in the field of brain state classification.</p>
<p>I trained a novel deep neural network ASRCNet to classify 14 brain states using data from MEG recordings. It can distinguish brain states under different image stimuli with an accuracy at least as good as, or even higher than the baseline pipeline. ASRCNet allows the extraction of spatial and temporal information from the informative MEG data. It is suggested that classification decisions are unlikely to be associated with activities that are unrelated to the task itself, for example, mind wandering. The trained ASRCNet successfully classifies brain states with relatively high accuracy and specificity. Previous research has been focusing on common symptoms of psychopathology, but less on the domain of aversive states. This is a study using MEG signals to classify different aversive brain states with a classifier. The high specificity for all states suggests that ASRCNet will help improve our understanding of the human cognitive image process. One thing we can do with such a CNN classifier is to look at state reactivation in cognitive tasks. Moreover, this classifier is expected to be applied in clinical studies in order to diagnose nonorganic neurological diseases. As shown in the topological map in the result section, these brain states are hard to classify with the naked eyes. The information from such computer-aided diagnosis may be a novel biomarker for these diseases in clinical practice.</p>
<p>The advantage of using this neural network is its comprehensive training process, entirely based on gradient descent-based optimization without intermediate steps. As research develops toward explainable artificial intelligence (XAI), the parameters of a model may be going to have a direct and explainable connection to their task. On the separate brain states classification task, ASRCNet also performs on par with state-of-the-art, potentially making it a general method for other neuroimaging data. What is more, as shown in the result section, the classification accuracy varies a lot between simple image classification CNN and ASRCNet. One reason the model is successful in classification is that the first part of the network learns to extract the correct features, while the last layer classifies the extracted features. It may suggest that the classification of such brain states depends on some event-independent temporal and spatial features signal. Some evidence suggests that using features automatically extracted with deep learning models rather than manually selected, is able to help achieve the highest levels of accuracy compared to other machine learning approaches. It is reported that most of the best ImageNet is achieved by using some kind of data augmentation, instead of feature engineering and dimensionality reduction (de Bardeci et al., 2021). However, it was reported that logistic regression may achieve better classification accuracy than ASRCNet (Wise et al., 2021). It may be because of the algorithmic Incompleteness of the current model. Future work on improving the algorithm may improve the performance of CNN models.</p>
<p>ASRCNet is a relatively robust approach among different subjects. In this research, 14 different stimuli were applied to 24 subjects (data from 4 subjects were removed because of the smaller segment length). ASRCNet successfully classifies states in 24 different subjects with high accuracy, demonstrating the robustness of the CNN model. However, the data itself used in the research has potential for improvement: the 13th stimulus is a face picture, which may be different from other stimuli reflecting brain states (Rapcsak, 2019). What is more, it is concerned that the trained model may have difficulty classifying brain states using data recorded by another MEG scanner. Improvements in current source estimation and alignment techniques may make the method adaptable to different MEG scanners (Pettersen et al., 2006). Apart from the robustness, the size of the data itself may also affect the classification accuracy. A limitation of this experiment is, the same as in most other studies, the cross-validation approach was performed during the validation process, rather than using a separate test dataset. It was reported that using a separate test set in the DL model may help yield the highest level of validity in the results (de Bardeci et al., 2021). Although superficially, it is a relatively advanced practice to use data from the same subject in the training and test sets, there is room for improvement. A possible improvement is to create additional test sets beyond the limited availability of data. Due to the high inter-individual specificity and intra-individual stability of MEG data, it is difficult for the network to learn common features between subjects. The current approach of the model is to recognize different subjects by identifying individual MEG features of different subjects. Therefore, even though the network can achieve high levels of accuracy, classification and prediction will be unpredictable when it is applied to entirely new datasets from different subjects. The application of transfer learning methods with the small tuning of part of model parameters may be a possible solution. However, when performing transfer learning, it is generally assumed that different tasks are related. In this case, how to define the correlation and mathematically describe the strength of the correlation between tasks are subjective decisions that are biased toward researchers. The image classification-related studies usually use ImageNet as a pre-trained model for transfer learning because the large dataset of ImageNet itself ensures that the trained model has high generalization. But when we use a small dataset such as in this experiment, transfer learning may not only fail to achieve the expected result but result in negative transferring which is even worse than training a network from nowhere. For example, AlphaGo Zero learned from zero without any supervision or using chess manual data but achieves higher performance than AlphaGo Lee which is based on chess manual replays (Silver et al., 2017). Therefore, how to perform correct and effective transfer learning is one of the focuses of future work.</p>
<p>Deep learning from scratch is often difficult with limited amounts of data. However, even with the limited amount of data in this study, I successfully classified 14 types of brain states. One reason for this success is that I enlarged the dataset by dividing each subject&#39;s 1170 seconds of data into 900 segments of 1.3-second time data, allowing me to train 14 classes using approximately 25200 segments. The data amount is slightly less than the amount of MNIST, a database of handwritten digits that is often used to train deep neural networks, but still shows that I have a reasonable amount of data to train a network of such size (Yann LeCun et al., 2012). However, during the training process, the training data generally has a tendency to overfit even though I applied dropout (randomly ignored neurons in network) and batch normalization (normalizes input mini-batches from last layer). The proposed CNN model ASRCNet may benefit from the increase in dataset size. It improves with more training data because deep learning performance is reported to improve significantly with larger datasets (Greenspan et al., 2016). Therefore, it can be taken into account to train this model with more data in hopes of improving model performance. In addition, more research has shown that MEG and EEG provide complementary information, and other modalities such as MRI also provide additional useful information. Using data from multiple method sources at the same time may improve model performance (Dale &amp; Sereno, 1993; Sharon et al., 2007). In future work, combining MEG signal data with EEG to create a multimodal data input may help improve the accuracy of brain states classification.</p>
<p>In this ASRCNet, the integration of the relative power spectrum improves the CNN model’s performance. Since the beta and delta waves mainly encode perceptual information, the relative power spectrum ensemble adds relevant information to the model so that the different band power values can inform the network. Therefore, the model can adjust the weights accordingly. Additionally, the relative power spectrum may also add valuable information about artefacts or ambient noise (Anelli et al., 2021). All these deep learning models overfit the training dataset more or less, even applied regularization techniques like dropout and batch normalization. The additional information provided by the relative power spectrum ensemble helps the model to generalize. There is still room to improve the model’s performance as the aspect of power spectrum extraction. In the process of relative power spectrum extraction, the wavelet transform can be considered as an optional alternative to Fourier analysis for the reason that the wavelet transform has the multi-scale analysis ability to extract features from the dataset and generate input images for training the model. Compared with the Fourier transform, the wavelet transform is a local transform of temporal and frequency data, so it can more effectively extract information from the signal by performing multi-scale refinement analysis with operations like scaling and translation (Yu &amp; Guowen, 1994), thus has the potential to solve some difficult problems that Fourier transform cannot deal with. Fourier transform can only get a frequency spectrum, but wavelet transform can get a temporal frequency spectrum which not only the frequency can be obtained, but also the time can be located. Some recent studies successfully use the wavelet packet decomposition method to extract time-frequency features and use a dynamic frequency feature selection algorithm to select the most accurate features for each subject (Luo et al., 2016). However, other studies have shown the drawback of wavelet packet decomposition: although this method improves the classification accuracy, it requires a lot of work to select the most suitable features for each subject, and the feature extraction for different target individuals is poorly general (Dai et al., 2020). Only considering the power spectrum is quite limiting at the current moment. The convolutional operation executed by most popular machine learning libraries in deep learning is actually computing the correlation measurement (Graves, 2012). Future work investigating state classification and reactivation should also take steps to measure event-related desynchronization and synchronization in the context of the data generated.</p>
<p>Another idea to optimize the model is to replace the fully connected layer with global average pooling. The model holds a redundancy of fully connected layer parameters where fully connected layer parameters can account for about 80% of the entire network parameters. Some recent network models with excellent performance, such as ResNet, are trying to use global average pooling (GAP) layer instead of a fully connected layer for fusion. For the learned deep features, loss functions such as SoftMax are still used as the network objective function to guide the learning process (Liu &amp; Zeng, 2022) Some evidence suggests that networks that replace a fully connected layer with a GAP layer may have better classification performance (Wei et al., 2018). However, other studies have pointed out that when applying transfer learning, the fine-tuned results of networks without fully connected layers are worse than those with fully connected layers. Therefore, fully connected can be regarded as a guard for model representation capabilities, especially in the case that there are big differences between the source domain and the target domain, the redundant parameters of FC can maintain a fine model capacity to ensure the migration of model representation capabilities (Zhang et al., 2018). Future work can explore the role of the GAP layer and fully connected layer in detail.</p>
<p>ASRCNet can extract and analyze features that deep learning neural networks use for classification, which may help researchers understand brain states better. The inherent hidden patterns in brain states and related brain neural activity that deep learning may reveal some fundamental mechanisms behind human behaviour. To the extent these can be explained, researchers are encouraged to apply these sophisticated deep learning modelling techniques to obtain accurate classification and prediction results and to generalize the results more carefully in a wider range of conditions. Moreover, understanding the relevant studies that extract these hidden patterns can increase and deepen our understanding of the brain state electrophysiological characterization.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>Anelli, M., Lauri, S. P., Advisor, P., &amp; Zubarev, M. I. (2021). <em>Using deep learning to predict continuous hand kinematics from magnetoencephalographic (MEG) measurements of electromagnetic brain activity.</em> <a href="http://www.aalto.fi" target="_blank" rel="noopener">www.aalto.fi</a></p>
<p>Aoe, J., Fukuma, R., Yanagisawa, T., Harada, T., Tanaka, M., Kobayashi, M., Inoue, Y., Yamamoto, S., Ohnishi, Y., &amp; Kishima, H. (2019). Automatic diagnosis of neurological diseases using MEG signals with a deep neural network. <em>Scientific Reports</em>, <em>9</em>(1). <a href="https://doi.org/10.1038/S41598-019-41500-X" target="_blank" rel="noopener">https://doi.org/10.1038/S41598-019-41500-X</a></p>
<p>Bai, S., Kolter, J. Z., &amp; Koltun, V. (2018). <em>An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</em>. <a href="https://doi.org/10.48550/arxiv.1803.01271" target="_blank" rel="noopener">https://doi.org/10.48550/arxiv.1803.01271</a></p>
<p>Baumeister, J., Barthel, T., Geiss, K. R., &amp; Weiss, M. (2013). Influence of phosphatidylserine on cognitive performance and cortical activity after induced stress. <em><a href="Http://Dx.Doi.Org/10.1179/147683008X301478" target="_blank" rel="noopener">Http://Dx.Doi.Org/10.1179/147683008X301478</a></em>, <em>11</em>(3), 103–110. <a href="https://doi.org/10.1179/147683008X301478" target="_blank" rel="noopener">https://doi.org/10.1179/147683008X301478</a></p>
<p>Belal, S., Cousins, J., El-Deredy, W., Parkes, L., Schneider, J., Tsujimura, H., Zoumpoulaki, A., Perapoch, M., Santamaria, L., &amp; Lewis, P. (2018). Identification of memory reactivation during sleep by EEG classification. <em>NeuroImage</em>, <em>176</em>, 203–214. <a href="https://doi.org/10.1016/J.NEUROIMAGE.2018.04.029" target="_blank" rel="noopener">https://doi.org/10.1016/J.NEUROIMAGE.2018.04.029</a></p>
<p>Boto, E., Holmes, N., Leggett, J., Roberts, G., Shah, V., Meyer, S. S., Muñoz, L. D., Mullinger, K. J., Tierney, T. M., Bestmann, S., Barnes, G. R., Bowtell, R., &amp; Brookes, M. J. (2018). Moving magnetoencephalography towards real-world applications with a wearable system. <em>Nature 2018 555:7698</em>, <em>555</em>(7698), 657–661. <a href="https://doi.org/10.1038/nature26147" target="_blank" rel="noopener">https://doi.org/10.1038/nature26147</a></p>
<p>Brown, R. E., &amp; Milner, P. M. (2003). The legacy of Donald O. Hebb: More than the Hebb Synapse. <em>Nature Reviews Neuroscience</em>, <em>4</em>(12), 1013–1019. <a href="https://doi.org/10.1038/NRN1257" target="_blank" rel="noopener">https://doi.org/10.1038/NRN1257</a></p>
<p>Bunge, S. A., &amp; Kahn, I. (2009). Cognition: An Overview of Neuroimaging Techniques. <em>Encyclopedia of Neuroscience</em>, 1063–1067. <a href="https://doi.org/10.1016/B978-008045046-9.00298-9" target="_blank" rel="noopener">https://doi.org/10.1016/B978-008045046-9.00298-9</a></p>
<p>Bzdok, D., &amp; Meyer-Lindenberg, A. (2018). Machine Learning for Precision Psychiatry: Opportunities and Challenges. <em>Biological Psychiatry. Cognitive Neuroscience and Neuroimaging</em>, <em>3</em>(3), 223–230. <a href="https://doi.org/10.1016/J.BPSC.2017.11.007" target="_blank" rel="noopener">https://doi.org/10.1016/J.BPSC.2017.11.007</a></p>
<p>Currie, G., Hawk, K. E., Rohren, E., Vial, A., &amp; Klein, R. (2019). Machine Learning and Deep Learning in Medical Imaging: Intelligent Imaging. <em>Journal of Medical Imaging and Radiation Sciences</em>, <em>50</em>(4), 477–487. <a href="https://doi.org/10.1016/j.jmir.2019.09.005" target="_blank" rel="noopener">https://doi.org/10.1016/j.jmir.2019.09.005</a></p>
<p>Dai, G., Zhou, J., Huang, J., &amp; Wang, N. (2020). HS-CNN: a CNN with hybrid convolution scale for EEG motor imagery classification. <em>Journal of Neural Engineering</em>, <em>17</em>(1), 016025. <a href="https://doi.org/10.1088/1741-2552/AB405F" target="_blank" rel="noopener">https://doi.org/10.1088/1741-2552/AB405F</a></p>
<p>Dale, A. M., &amp; Sereno, M. I. (1993). Improved Localizadon of Cortical Activity by Combining EEG and MEG with MRI Cortical Surface Reconstruction: A Linear Approach. <em>Journal of Cognitive Neuroscience</em>, <em>5</em>(2), 162–176. <a href="https://doi.org/10.1162/JOCN.1993.5.2.162" target="_blank" rel="noopener">https://doi.org/10.1162/JOCN.1993.5.2.162</a></p>
<p>Dash, D., Ferrari, P., Heitzman, D., &amp; Wang, J. (2019). Decoding Speech from Single Trial MEG Signals Using Convolutional Neural Networks and Transfer Learning. <em>Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS</em>, 5531–5535. <a href="https://doi.org/10.1109/EMBC.2019.8857874" target="_blank" rel="noopener">https://doi.org/10.1109/EMBC.2019.8857874</a></p>
<p>Dash, D., Sao, A. K., Wang, J., &amp; Biswal, B. (2019). How many fmri scans are necessary and sufficient for resting brain connectivity analysis? <em>2018 IEEE Global Conference on Signal and Information Processing, GlobalSIP 2018 - Proceedings</em>, 494–498. <a href="https://doi.org/10.1109/GLOBALSIP.2018.8646415" target="_blank" rel="noopener">https://doi.org/10.1109/GLOBALSIP.2018.8646415</a></p>
<p>de Bardeci, M., Ip, C. T., &amp; Olbrich, S. (2021). Deep learning applied to electroencephalogram data in mental disorders: A systematic review. <em>Biological Psychology</em>, <em>162</em>, 108117. <a href="https://doi.org/10.1016/J.BIOPSYCHO.2021.108117" target="_blank" rel="noopener">https://doi.org/10.1016/J.BIOPSYCHO.2021.108117</a></p>
<p>Doll, B. B., Duncan, K. D., Simon, D. A., Shohamy, D., &amp; Daw, N. D. (2015). Model-based choices involve prospective neural activity. <em>Nature Neuroscience</em>, <em>18</em>(5), 767. <a href="https://doi.org/10.1038/NN.3981" target="_blank" rel="noopener">https://doi.org/10.1038/NN.3981</a></p>
<p>Eichenlaub, J. B., Biswal, S., Peled, N., Rivilis, N., Golby, A. J., Lee, J. W., Westover, M. B., Halgren, E., &amp; Cash, S. S. (2020). Reactivation of Motor-Related Gamma Activity in Human NREM Sleep. <em>Frontiers in Neuroscience</em>, <em>14</em>. <a href="https://doi.org/10.3389/FNINS.2020.00449" target="_blank" rel="noopener">https://doi.org/10.3389/FNINS.2020.00449</a></p>
<p>Einöther, S. J. L., Giesbrecht, T., Walden, C. M., van Buren, L., van der Pijl, P. C., &amp; de Bruin, E. A. (2013). Attention Benefits of Tea and Tea Ingredients: A Review of the Research to Date. <em>Tea in Health and Disease Prevention</em>, 1373–1384. <a href="https://doi.org/10.1016/B978-0-12-384937-3.00115-4" target="_blank" rel="noopener">https://doi.org/10.1016/B978-0-12-384937-3.00115-4</a></p>
<p>Faraji, I., Mirsadeghi, S. H., &amp; Afsahi, A. (2016). Topology-aware GPU selection on multi-GPU nodes. <em>Proceedings - 2016 IEEE 30th International Parallel and Distributed Processing Symposium, IPDPS 2016</em>, 712–720. <a href="https://doi.org/10.1109/IPDPSW.2016.44" target="_blank" rel="noopener">https://doi.org/10.1109/IPDPSW.2016.44</a></p>
<p>Giovannetti, A., Susi, G., Casti, P., Mencattini, A., Pusil, S., López, M. E., di Natale, C., &amp; Martinelli, E. (2021). Deep-MEG: spatiotemporal CNN features and multiband ensemble classification for predicting the early signs of Alzheimer’s disease with magnetoencephalography. <em>Neural Computing and Applications</em>, <em>33</em>(21), 14651–14667. <a href="https://doi.org/10.1007/S00521-021-06105-4/TABLES/4" target="_blank" rel="noopener">https://doi.org/10.1007/S00521-021-06105-4/TABLES/4</a></p>
<p>Gramfort, A., Luessi, M., Larson, E., Engemann, D. A., Strohmeier, D., Brodbeck, C., Parkkonen, L., &amp; Hämäläinen, M. S. (2014). MNE software for processing MEG and EEG data. <em>NeuroImage</em>, <em>86</em>, 446–460. <a href="https://doi.org/10.1016/J.NEUROIMAGE.2013.10.027" target="_blank" rel="noopener">https://doi.org/10.1016/J.NEUROIMAGE.2013.10.027</a></p>
<p>Graves, A. (2012). <em>Supervised Sequence Labelling</em>. 5–13. <a href="https://doi.org/10.1007/978-3-642-24797-2_2" target="_blank" rel="noopener">https://doi.org/10.1007/978-3-642-24797-2_2</a></p>
<p>Greenspan, H., van Ginneken, B., &amp; Summers, R. M. (2016). Guest Editorial Deep Learning in Medical Imaging: Overview and Future Promise of an Exciting New Technique. <em>IEEE Transactions on Medical Imaging</em>, <em>35</em>(5), 1153–1159. <a href="https://doi.org/10.1109/TMI.2016.2553401" target="_blank" rel="noopener">https://doi.org/10.1109/TMI.2016.2553401</a></p>
<p>Hagemann, D., Hewig, J., Walter, C., &amp; Naumann, E. (2008). Skull thickness and magnitude of EEG alpha activity. <em>Clinical Neurophysiology</em>, <em>119</em>(6), 1271–1280. <a href="https://doi.org/10.1016/J.CLINPH.2008.02.010" target="_blank" rel="noopener">https://doi.org/10.1016/J.CLINPH.2008.02.010</a></p>
<p>Hardt, M., Recht, B., &amp; Singer, Y. (2015). Train faster, generalize better: Stability of stochastic gradient descent. <em>33rd International Conference on Machine Learning, ICML 2016</em>, <em>3</em>, 1868–1877. <a href="https://doi.org/10.48550/arxiv.1509.01240" target="_blank" rel="noopener">https://doi.org/10.48550/arxiv.1509.01240</a></p>
<p>Haynes, J. D. (2012). Brain reading. <em>I Know What You’re Thinking: Brain Imaging and Mental Privacy</em>. <a href="https://doi.org/10.1093/ACPROF:OSO/9780199596492.003.0003" target="_blank" rel="noopener">https://doi.org/10.1093/ACPROF:OSO/9780199596492.003.0003</a></p>
<p>Hedderich, D. M., &amp; Eickhoff, S. B. (2021). Machine learning for psychiatry: getting doctors at the black box? <em>Molecular Psychiatry</em>, <em>26</em>(1), 23. <a href="https://doi.org/10.1038/S41380-020-00931-Z" target="_blank" rel="noopener">https://doi.org/10.1038/S41380-020-00931-Z</a></p>
<p>Hoshen, Y., Weiss, R. J., &amp; Wilson, K. W. (2015). Speech acoustic modeling from raw multichannel waveforms. <em>ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings</em>, <em>2015-August</em>, 4624–4628. <a href="https://doi.org/10.1109/ICASSP.2015.7178847" target="_blank" rel="noopener">https://doi.org/10.1109/ICASSP.2015.7178847</a></p>
<p>Huang, W., Yan, H., Wang, C., Li, J., Yang, X., Li, L., Zuo, Z., Zhang, J., &amp; Chen, H. (2020). Long short-term memory-based neural decoding of object categories evoked by natural images. <em>Human Brain Mapping</em>, <em>41</em>(15), 4442–4453. <a href="https://doi.org/10.1002/hbm.25136" target="_blank" rel="noopener">https://doi.org/10.1002/hbm.25136</a></p>
<p>Huang, Y., Sun, X., Lu, M., &amp; Xu, M. (2015). Channel-Max, Channel-Drop and Stochastic Max-pooling. <em>IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</em>, <em>2015-October</em>, 9–17. <a href="https://doi.org/10.1109/CVPRW.2015.7301267" target="_blank" rel="noopener">https://doi.org/10.1109/CVPRW.2015.7301267</a></p>
<p>IBM Corp. (2021). <em>IBM SPSS Statistics for Windows</em>. <a href="https://hadoop.apache.org" target="_blank" rel="noopener">https://hadoop.apache.org</a></p>
<p>Ioffe, S., &amp; Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. <em>32nd International Conference on Machine Learning, ICML 2015</em>, <em>1</em>, 448–456. <a href="https://doi.org/10.48550/arxiv.1502.03167" target="_blank" rel="noopener">https://doi.org/10.48550/arxiv.1502.03167</a></p>
<p>Karimpanal, T. G., &amp; Bouffanais, R. (2018). Self-Organizing Maps for Storage and Transfer of Knowledge in Reinforcement Learning. <em>Adaptive Behavior</em>, <em>27</em>(2), 111–126. <a href="https://doi.org/10.1177/1059712318818568" target="_blank" rel="noopener">https://doi.org/10.1177/1059712318818568</a></p>
<p>Khan, S., Rahmani, H., Shah, S. A. A., &amp; Bennamoun, M. (2018). A Guide to Convolutional Neural Networks for Computer Vision. <em>A Guide to Convolutional Neural Networks for Computer Vision</em>. <a href="https://doi.org/10.1007/978-3-031-01821-3" target="_blank" rel="noopener">https://doi.org/10.1007/978-3-031-01821-3</a></p>
<p>King’s College London. (2022). <em>King’s Computational Research, Engineering and Technology Environment (CREATE).</em></p>
<p>Li, H., Ellis, J. G., Zhang, L., &amp; Chang, S. F. (2018). PatternNet: Visual pattern mining with deep neural network. <em>ICMR 2018 - Proceedings of the 2018 ACM International Conference on Multimedia Retrieval</em>, 291–299. <a href="https://doi.org/10.1145/3206025.3206039" target="_blank" rel="noopener">https://doi.org/10.1145/3206025.3206039</a></p>
<p>Liu, W., &amp; Zeng, Y. (2022). Motor Imagery Tasks EEG Signals Classification Using ResNet with Multi-Time-Frequency Representation. <em>2022 7th International Conference on Intelligent Computing and Signal Processing, ICSP 2022</em>, 2026–2029. <a href="https://doi.org/10.1109/ICSP54964.2022.9778786" target="_blank" rel="noopener">https://doi.org/10.1109/ICSP54964.2022.9778786</a></p>
<p>Luo, J., Feng, Z., Zhang, J., &amp; Lu, N. (2016). Dynamic frequency feature selection based approach for classification of motor imageries. <em>Computers in Biology and Medicine</em>, <em>75</em>, 45–53. <a href="https://doi.org/10.1016/J.COMPBIOMED.2016.03.004" target="_blank" rel="noopener">https://doi.org/10.1016/J.COMPBIOMED.2016.03.004</a></p>
<p>Newson, J. J., &amp; Thiagarajan, T. C. (2019). EEG Frequency Bands in Psychiatric Disorders: A Review of Resting State Studies. <em>Frontiers in Human Neuroscience</em>, <em>12</em>, 521. <a href="https://doi.org/10.3389/FNHUM.2018.00521/BIBTEX" target="_blank" rel="noopener">https://doi.org/10.3389/FNHUM.2018.00521/BIBTEX</a></p>
<p>Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., Facebook, Z. D., Research, A. I., Lin, Z., Desmaison, A., Antiga, L., Srl, O., &amp; Lerer, A. (2017). <em>Automatic differentiation in PyTorch</em>.</p>
<p>Percival, D. B., &amp; Walden, A. T. (1993). Spectral Analysis for Physical Applications. <em>Spectral Analysis for Physical Applications</em>. <a href="https://doi.org/10.1017/CBO9780511622762" target="_blank" rel="noopener">https://doi.org/10.1017/CBO9780511622762</a></p>
<p>Pettersen, K. H., Devor, A., Ulbert, I., Dale, A. M., &amp; Einevoll, G. T. (2006). Current-source density estimation based on inversion of electrostatic forward solution: Effects of finite extent of neuronal activity and conductivity discontinuities. <em>Journal of Neuroscience Methods</em>, <em>154</em>(1–2), 116–133. <a href="https://doi.org/10.1016/J.JNEUMETH.2005.12.005" target="_blank" rel="noopener">https://doi.org/10.1016/J.JNEUMETH.2005.12.005</a></p>
<p>Rajasree, R., Columbus, C. C., &amp; Shilaja, C. (2021). Multiscale-based multimodal image classification of brain tumor using deep learning method. <em>Neural Computing and Applications</em>, <em>33</em>(11), 5543–5553. <a href="https://doi.org/10.1007/S00521-020-05332-5/FIGURES/9" target="_blank" rel="noopener">https://doi.org/10.1007/S00521-020-05332-5/FIGURES/9</a></p>
<p>Rapcsak, S. Z. (2019). Face Recognition. <em>Current Neurology and Neuroscience Reports</em>, <em>19</em>(7). <a href="https://doi.org/10.1007/S11910-019-0960-9" target="_blank" rel="noopener">https://doi.org/10.1007/S11910-019-0960-9</a></p>
<p>RaviPrakash, H., Korostenskaja, M., Castillo, E. M., Lee, K. H., Salinas, C. M., Baumgartner, J., Anwar, S. M., Spampinato, C., &amp; Bagci, U. (2020). Deep Learning Provides Exceptional Accuracy to ECoG-Based Functional Language Mapping for Epilepsy Surgery. <em>Frontiers in Neuroscience</em>, <em>14</em>. <a href="https://doi.org/10.3389/FNINS.2020.00409/FULL" target="_blank" rel="noopener">https://doi.org/10.3389/FNINS.2020.00409/FULL</a></p>
<p>Rohenkohl, G., &amp; Nobre, A. C. (2011). Alpha Oscillations Related to Anticipatory Attention Follow Temporal Expectations. <em>The Journal of Neuroscience</em>, <em>31</em>(40), 14076. <a href="https://doi.org/10.1523/JNEUROSCI.3387-11.2011" target="_blank" rel="noopener">https://doi.org/10.1523/JNEUROSCI.3387-11.2011</a></p>
<p>Roscow, E. L., Chua, R., Costa, R. P., Jones, M. W., &amp; Lepora, N. (2021). Learning offline: memory replay in biological and artificial reinforcement learning. <em>Trends in Neurosciences</em>, <em>44</em>(10), 808–821. <a href="https://doi.org/10.1016/J.TINS.2021.07.007" target="_blank" rel="noopener">https://doi.org/10.1016/J.TINS.2021.07.007</a></p>
<p>Schacter, D. L., Benoit, R. G., &amp; Szpunar, K. K. (2017). Episodic Future Thinking: Mechanisms and Functions. <em>Current Opinion in Behavioral Sciences</em>, <em>17</em>, 41. <a href="https://doi.org/10.1016/J.COBEHA.2017.06.002" target="_blank" rel="noopener">https://doi.org/10.1016/J.COBEHA.2017.06.002</a></p>
<p>Sharon, D., Hämäläinen, M. S., Tootell, R. B. H., Halgren, E., &amp; Belliveau, J. W. (2007). The advantage of combining MEG and EEG: Comparison to fMRI in focally stimulated visual cortex. <em>NeuroImage</em>, <em>36</em>(4), 1225–1235. <a href="https://doi.org/10.1016/J.NEUROIMAGE.2007.03.066" target="_blank" rel="noopener">https://doi.org/10.1016/J.NEUROIMAGE.2007.03.066</a></p>
<p>Shaw, G. L. (1986). Donald Hebb: The Organization of Behavior. <em>Brain Theory</em>, 231–233. <a href="https://doi.org/10.1007/978-3-642-70911-1_15" target="_blank" rel="noopener">https://doi.org/10.1007/978-3-642-70911-1_15</a></p>
<p>Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L., van den Driessche, G., Graepel, T., &amp; Hassabis, D. (2017). Mastering the game of Go without human knowledge. <em>Nature 2017 550:7676</em>, <em>550</em>(7676), 354–359. <a href="https://doi.org/10.1038/nature24270" target="_blank" rel="noopener">https://doi.org/10.1038/nature24270</a></p>
<p>Singh, S. P. (2014). Magnetoencephalography: Basic principles. <em>Annals of Indian Academy of Neurology</em>, <em>17</em>(Suppl 1), S107. <a href="https://doi.org/10.4103/0972-2327.128676" target="_blank" rel="noopener">https://doi.org/10.4103/0972-2327.128676</a></p>
<p>Slepian, D. (1978). Prolate Spheroidal Wave Functions, Fourier Analysis, and Uncertainty—V: The Discrete Case. <em>Bell System Technical Journal</em>, <em>57</em>(5), 1371–1430. <a href="https://doi.org/10.1002/J.1538-7305.1978.TB02104.X" target="_blank" rel="noopener">https://doi.org/10.1002/J.1538-7305.1978.TB02104.X</a></p>
<p>Soria Olivas, E., &amp; IGI Global. (2010). <em>Handbook of research on machine learning applications and trends : algorithms, methods, and techniques</em>. 83.</p>
<p>Srivastava, N., Hinton, G., Krizhevsky, A., &amp; Salakhutdinov, R. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. <em>Journal of Machine Learning Research</em>, <em>15</em>, 1929–1958. <a href="https://doi.org/10.5555/2627435" target="_blank" rel="noopener">https://doi.org/10.5555/2627435</a></p>
<p>Szpunar, K. K. (2010). Episodic Future Thought: An Emerging Concept. <em>Perspectives on Psychological Science : A Journal of the Association for Psychological Science</em>, <em>5</em>(2), 142–162. <a href="https://doi.org/10.1177/1745691610362350" target="_blank" rel="noopener">https://doi.org/10.1177/1745691610362350</a></p>
<p>Tappert, C. C. (2019). Who is the father of deep learning? <em>Proceedings - 6th Annual Conference on Computational Science and Computational Intelligence, CSCI 2019</em>, 343–348. <a href="https://doi.org/10.1109/CSCI49370.2019.00067" target="_blank" rel="noopener">https://doi.org/10.1109/CSCI49370.2019.00067</a></p>
<p>Terranova, J. I., Yokose, J., Osanai, H., Marks, W. D., Yamamoto, J., Ogawa, S. K., &amp; Kitamura, T. (2022). Hippocampal-amygdala memory circuits govern experience-dependent observational fear. <em>Neuron</em>, <em>110</em>(8), 1416-1431.e13. <a href="https://doi.org/10.1016/J.NEURON.2022.01.019" target="_blank" rel="noopener">https://doi.org/10.1016/J.NEURON.2022.01.019</a></p>
<p>Tokozume, Y., &amp; Harada, T. (2017). Learning environmental sounds with end-to-end convolutional neural network. <em>ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings</em>, 2721–2725. <a href="https://doi.org/10.1109/ICASSP.2017.7952651" target="_blank" rel="noopener">https://doi.org/10.1109/ICASSP.2017.7952651</a></p>
<p>Tokozume, Y., Ushiku, Y., &amp; Harada, T. (2017). Learning from Between-class Examples for Deep Sound Recognition. <em>6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings</em>. <a href="https://doi.org/10.48550/arxiv.1711.10282" target="_blank" rel="noopener">https://doi.org/10.48550/arxiv.1711.10282</a></p>
<p>Valueva, M. v., Nagornov, N. N., Lyakhov, P. A., Valuev, G. v., &amp; Chervyakov, N. I. (2020). Application of the residue number system to reduce hardware costs of the convolutional neural network implementation. <em>Mathematics and Computers in Simulation</em>, <em>177</em>, 232–243. <a href="https://doi.org/10.1016/J.MATCOM.2020.04.031" target="_blank" rel="noopener">https://doi.org/10.1016/J.MATCOM.2020.04.031</a></p>
<p>Wei, X. S., Zhang, C. L., Zhang, H., &amp; Wu, J. (2018). Deep Bimodal Regression of Apparent Personality Traits from Short Video Sequences. <em>IEEE Transactions on Affective Computing</em>, <em>9</em>(3), 303–315. <a href="https://doi.org/10.1109/TAFFC.2017.2762299" target="_blank" rel="noopener">https://doi.org/10.1109/TAFFC.2017.2762299</a></p>
<p>Wimmer, G. E., &amp; Shohamy, D. (2012). Preference by association: how memory mechanisms in the hippocampus bias decisions. <em>Science (New York, N.Y.)</em>, <em>338</em>(6104), 270–273. <a href="https://doi.org/10.1126/SCIENCE.1223252" target="_blank" rel="noopener">https://doi.org/10.1126/SCIENCE.1223252</a></p>
<p>Wise, T., Liu, Y., Chowdhury, F., &amp; Dolan, R. J. (2021). Model-based aversive learning in humans is supported by preferential task state reactivation. <em>Science Advances</em>, <em>7</em>(31), 9616–9644. <a href="https://doi.org/10.1126/SCIADV.ABF9616" target="_blank" rel="noopener">https://doi.org/10.1126/SCIADV.ABF9616</a></p>
<p>Wu, C. T., Haggerty, D., Kemere, C., &amp; Ji, D. (2017). Hippocampal awake replay in fear memory retrieval. <em>Nature Neuroscience</em>, <em>20</em>(4), 571. <a href="https://doi.org/10.1038/NN.4507" target="_blank" rel="noopener">https://doi.org/10.1038/NN.4507</a></p>
<p>Yann LeCun, Corinna Cortes, &amp; Chris Burges. (2012). <em>MNIST handwritten digit database</em>. <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/</a></p>
<p>Yu, F. T. S., &amp; Guowen, L. (1994). Short-time Fourier transform and wavelet transform with Fourier-domain processing. <em>Applied Optics</em>, <em>33</em>(23), 5262–5270. <a href="https://doi.org/10.1364/AO.33.005262" target="_blank" rel="noopener">https://doi.org/10.1364/AO.33.005262</a></p>
<p>Zhang, C. L., Luo, J. H., Wei, X. S., &amp; Wu, J. (2018). In defense of fully connected layers in visual representation transfer. <em>Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</em>, <em>10736 LNCS</em>, 807–817. <a href="https://doi.org/10.1007/978-3-319-77383-4_79/TABLES/4" target="_blank" rel="noopener">https://doi.org/10.1007/978-3-319-77383-4_79/TABLES/4</a></p>
<p>Zheng, L., Liao, P., Luo, S., Sheng, J., Teng, P., Luan, G., &amp; Gao, J. H. (2020). EMS-Net: A Deep Learning Method for Autodetecting Epileptic Magnetoencephalography Spikes. <em>IEEE Transactions on Medical Imaging</em>, <em>39</em>(6), 1833–1844. <a href="https://doi.org/10.1109/TMI.2019.2958699" target="_blank" rel="noopener">https://doi.org/10.1109/TMI.2019.2958699</a></p>
<p>Zubarev, I., Zetter, R., Halme, H. L., &amp; Parkkonen, L. (2019). Adaptive neural network classifier for decoding MEG signals. <em>NeuroImage</em>, <em>197</em>, 425–434. <a href="https://doi.org/10.1016/J.NEUROIMAGE.2019.04.068" target="_blank" rel="noopener">https://doi.org/10.1016/J.NEUROIMAGE.2019.04.068</a></p>

            </div>
            <hr />

            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone, qq, weibo, douban"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            

    <div class="reprint" id="reprint-statement">
        <p class="reprint-tip">
            <i class="fa fa-exclamation-triangle"></i>&nbsp;&nbsp;
            <span>Reprint policy</span>
        </p>
        
            <div class="center-align">
                <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">
                    <img alt=""
                         style="border-width:0"
                         src="https://i.creativecommons.org/l/by/4.0/88x31.png"/>
                </a>
            </div>
            <br/>
            <span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text"
                  property="dct:title" rel="dct:type">
                    《Thesis》
                </span> by
            <a xmlns:cc="http://creativecommons.org/ns#" href="/2022/08/24/thesis/" property="cc:attributionName"
               rel="cc:attributionURL">
                Lei Luo
            </a> is licensed under a
            <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">
                Creative Commons Attribution 4.0 International License
            </a> 
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Copied successfully, please follow the reprint policy of this article</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">more</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>


        </div>
    </div>

    

    

    

    

    
    <style>
    .valine-card {
        margin: 1.5rem auto;
    }

    .valine-card .card-content {
        padding: 20px 20px 5px 20px;
    }

    #vcomments input[type=text],
    #vcomments input[type=email],
    #vcomments input[type=url],
    #vcomments textarea {
        box-sizing: border-box;
    }

    #vcomments p {
        margin: 2px 2px 10px;
        font-size: 1.05rem;
        line-height: 1.78rem;
    }

    #vcomments blockquote p {
        text-indent: 0.2rem;
    }

    #vcomments a {
        padding: 0 2px;
        color: #42b983;
        font-weight: 500;
        text-decoration: underline;
    }

    #vcomments img {
        max-width: 100%;
        height: auto;
        cursor: pointer;
    }

    #vcomments ol li {
        list-style-type: decimal;
    }

    #vcomments ol,
    ul {
        display: block;
        padding-left: 2em;
        word-spacing: 0.05rem;
    }

    #vcomments ul li,
    ol li {
        display: list-item;
        line-height: 1.8rem;
        font-size: 1rem;
    }

    #vcomments ul li {
        list-style-type: disc;
    }

    #vcomments ul ul li {
        list-style-type: circle;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    #vcomments table, th, td {
        border: 0;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments h1 {
        font-size: 1.85rem;
        font-weight: bold;
        line-height: 2.2rem;
    }

    #vcomments h2 {
        font-size: 1.65rem;
        font-weight: bold;
        line-height: 1.9rem;
    }

    #vcomments h3 {
        font-size: 1.45rem;
        font-weight: bold;
        line-height: 1.7rem;
    }

    #vcomments h4 {
        font-size: 1.25rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    #vcomments h5 {
        font-size: 1.1rem;
        font-weight: bold;
        line-height: 1.4rem;
    }

    #vcomments h6 {
        font-size: 1rem;
        line-height: 1.3rem;
    }

    #vcomments p {
        font-size: 1rem;
        line-height: 1.5rem;
    }

    #vcomments hr {
        margin: 12px 0;
        border: 0;
        border-top: 1px solid #ccc;
    }

    #vcomments blockquote {
        margin: 15px 0;
        border-left: 5px solid #42b983;
        padding: 1rem 0.8rem 0.3rem 0.8rem;
        color: #666;
        background-color: rgba(66, 185, 131, .1);
    }

    #vcomments pre {
        font-family: monospace, monospace;
        padding: 1.2em;
        margin: .5em 0;
        background: #272822;
        overflow: auto;
        border-radius: 0.3em;
        tab-size: 4;
    }

    #vcomments code {
        font-family: monospace, monospace;
        padding: 1px 3px;
        font-size: 0.92rem;
        color: #e96900;
        background-color: #f8f8f8;
        border-radius: 2px;
    }

    #vcomments pre code {
        font-family: monospace, monospace;
        padding: 0;
        color: #e8eaf6;
        background-color: #272822;
    }

    #vcomments pre[class*="language-"] {
        padding: 1.2em;
        margin: .5em 0;
    }

    #vcomments code[class*="language-"],
    pre[class*="language-"] {
        color: #e8eaf6;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }

    #vcomments b,
    strong {
        font-weight: bold;
    }

    #vcomments dfn {
        font-style: italic;
    }

    #vcomments small {
        font-size: 85%;
    }

    #vcomments cite {
        font-style: normal;
    }

    #vcomments mark {
        background-color: #fcf8e3;
        padding: .2em;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }
</style>

<div class="card valine-card" data-aos="fade-up">
    <div id="vcomments" class="card-content"></div>
</div>

<script src="/libs/valine/av-min.js"></script>
<script src="/libs/valine/Valine.min.js"></script>
<!-- <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script> -->

<script>
    new Valine({
        el: '#vcomments',
        appId: '查教程获取',
        appKey: '查教程获取',
        notify: 'false' === 'true',
        verify: 'false' === 'true',
        visitor: 'false' === 'true',
        avatar: 'wavatar',
        pageSize: '10',
        lang: 'en',
        placeholder: 'Just go go'
    });
</script>

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fa fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2022/09/08/albania/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/6.jpg" class="responsive-img" alt="Albania 7 days">
                        
                        <span class="card-title">Albania 7 days</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            阿尔巴尼亚前期准备
机票 EasyJet
租车
住宿 萨兰达
住宿 都拉斯
住宿 蒂瓦特
住宿 佩拉斯特
住宿 地拉那
换汇？（列克）

Hint: 

在访问与科索沃北部边境的山区城镇时，您应该谨慎行事，并注意有关未爆地雷和其他未爆弹药的
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="fa fa-clock-o fa-fw icon-date"></i>2022-09-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Traveling/" class="post-category" target="_blank">
                                    Traveling
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Albania/" target="_blank">
                        <span class="chip bg-color">Albania</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fa fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2022/08/22/pandas/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/17.jpg" class="responsive-img" alt="Pandas">
                        
                        <span class="card-title">Pandas</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            The objective is to calculate power spectrum for each frequencies and output data to excel files.
First load packages:
i
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2022-08-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/programming/" class="post-category" target="_blank">
                                    programming
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Pandas/" target="_blank">
                        <span class="chip bg-color">Pandas</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + 'From: Lei's Blog<br />'
            + 'Author: Lei Luo<br />'
            + 'Link: <a href="' + url + '">' + url + '</a><br />'
            + 'The copyright of this article belongs to the author, please indicate the source for any form of reprint。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () { bodyElement.removeChild(newdiv); }, 200);
    });
</script>

    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="fa fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fa fa-list"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h1, h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).slideUp(500);
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).slideDown(500);
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>

<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>
<!-- 代码语言 -->
<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>
<!-- 代码块复制 -->
<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>
<script type="text/javascript" src="/libs/codeBlock/clipboard.min.js"></script>
<!-- 代码块收缩 -->
<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script> 
<!-- 代码块折行 -->
<style type="text/css">code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }</style>


    <footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            

            
            &nbsp;<i class="fa fa-area-chart"></i>&nbsp;Total words:&nbsp;
            <span class="white-color">50.8k</span>
            

            <br>
            <span id="sitetime"></span>

            
            
            <br>
            
            <span id="busuanzi_container_site_pv" style='display:none'>
                <i class="fa fa-heart-o"></i>
                Total visits <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
            <span id="busuanzi_container_site_uv" style='display:none'>
                times,&nbsp;visitors <span id="busuanzi_value_site_uv" class="white-color"></span> persons.
            </span>
            
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/ReveRoyl" class="tooltipped" target="_blank" data-tooltip="visit my GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:roylvpn@gmail.com" class="tooltipped" target="_blank" data-tooltip="email me" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>










    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS subscribe" data-position="top" data-delay="50">
        <i class="fa fa-rss"></i>
    </a>
</div>
    </div>
</footer>

<div class="progress-bar"></div>

<!-- 不蒜子计数初始值纠正 -->
<script>
    $(document).ready(function () {

        var int = setInterval(fixCount, 50);
        var pvcountOffset = 80000;
        var uvcountOffset = 20000;

        function fixCount() {
            if (document.getElementById("busuanzi_container_site_pv").style.display != "none") {
                $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + pvcountOffset);
                clearInterval(int);
            }
            if ($("#busuanzi_container_site_pv").css("display") != "none") {
                $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + uvcountOffset); // 加上初始数据 
                clearInterval(int);
            }
        }
    });
</script>

<script language=javascript>
    function siteTime() {
        window.setTimeout("siteTime()", 1000);
        var seconds = 1000;
        var minutes = seconds * 60;
        var hours = minutes * 60;
        var days = hours * 24;
        var years = days * 365;
        var today = new Date();
        var todayYear = today.getFullYear();
        var todayMonth = today.getMonth() + 1;
        var todayDate = today.getDate();
        var todayHour = today.getHours();
        var todayMinute = today.getMinutes();
        var todaySecond = today.getSeconds();
        /* Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
        year - 作为date对象的年份，为4位年份值
        month - 0-11之间的整数，做为date对象的月份
        day - 1-31之间的整数，做为date对象的天数
        hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
        minutes - 0-59之间的整数，做为date对象的分钟数
        seconds - 0-59之间的整数，做为date对象的秒数
        microseconds - 0-999之间的整数，做为date对象的毫秒数 */
        var t1 = Date.UTC(2021, 10, 28, 00, 00, 00); //北京时间2018-2-13 00:00:00
        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
        var diff = t2 - t1;
        var diffYears = Math.floor(diff / years);
        var diffDays = Math.floor((diff / days) - diffYears * 365);
        var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
        var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) / minutes);
        var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours - diffMinutes * minutes) / seconds);
        document.getElementById("sitetime").innerHTML = "This site has been running for " + diffYears + " year(s) " + diffDays + " day(s) " + diffHours + " hour(s) " + diffMinutes + " minute(s) " + diffSeconds + " second(s)";
    }/*因为建站时间还没有一年，就将之注释掉了。需要的可以取消*/
    siteTime();
</script>

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fa fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <script type="text/javascript"> var OriginTitile = document.title, st; document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ Oops！", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪ You're back！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) })
    </script>

    <!-- Global site tag (gtag.js) - Google Analytics -->

<script async src="https://www.googletagmanager.com/gtag/js?id=G-0R09SZV95V"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());
    gtag('config', 'G-0R09SZV95V');
</script>



    
    <script src="/libs/others/clicklove.js"></script>
    

    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    <!-- 雪花特效 -->
    

</body>

</html>